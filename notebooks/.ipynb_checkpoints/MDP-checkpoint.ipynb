{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$S_t$ Es el estado en el tiempo $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Processes\n",
    "\n",
    "\n",
    "[David Silver Lecture 2 MDP](https://www.youtube.com/watch?v=lfHX2hHRMVQ)\n",
    "\n",
    "## Notas\n",
    "\n",
    "* Los MDP describen un enviroment totalmente observable, es decir podemos acceder a toda la información del sistema.\n",
    "* La idea central es un MDP es que cumple la **propiedad de Markov**, esta dice que \"El futuro es independiente del pasado\".\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{P}[S_{t+1}|S_t] = \\mathbb{P}[S_{t+1}|S_1,...,S_t]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "* El estado $S_t$ caracteriza todo lo que necesitamos saber. Es suficiente estadistica del futuro.\n",
    "\n",
    "La probabilidad de un estado $s$ y un estado sucesor $s'$ viene dada por la probabilidad de transición de estado\n",
    "\n",
    "$$\n",
    "\\mathcal{P}_{ss'} = \\mathbb{P}[S_{t+1}=s'| S_t = s]\n",
    "$$\n",
    "\n",
    "\n",
    "Un proceso de markov es una secuencia de estados $S_1$, $S_2$, ... que cumplen la **propiedad de Markov**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Markov Reward Process** o **MRP**\n",
    "[Video Rewards](https://youtu.be/lfHX2hHRMVQ?t=784)\n",
    "\n",
    "La función de rewards nos da el reward de estar en el estado $S_t$, el reward lo recibimos al **salir** del estado\n",
    "\n",
    "$$\n",
    "\\mathcal{R}_s = \\mathbb{E}[R_{t+1}|S_t = s] \n",
    "$$\n",
    "\n",
    "Buscamos maximizar los rewards a lo largo de tiempo, esto se define con el **return** $G_t$ y el factor de descuento $\\gamma$ $\\in$ [0,1]\n",
    "\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + ... = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}\n",
    "$$\n",
    "\n",
    "La **state-value function** $v(s)$ nos da el valor a futuro del estado $s$\n",
    "$$\n",
    "v(s) = \\mathbb{E}[G_t | S_t=s]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Bellman Equation](https://youtu.be/lfHX2hHRMVQ?t=2345)\n",
    "$$\n",
    "v = \\mathcal{R}+\\gamma \\mathcal{P}v\n",
    "$$\n",
    "Con la solución\n",
    "\n",
    "$$\n",
    "v = (I - \\gamma \\mathcal{P})^{-1} \\mathcal{R}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MDP](https://youtu.be/lfHX2hHRMVQ?t=2590) Markov Decision Process\n",
    "\n",
    "\n",
    "Ahora la probabilidad de transición viene dada por en función de una acción $\\mathcal{A}$\n",
    "\n",
    "$$\n",
    "P_{ss'}^a = \\mathbb{P}[S_{t+1} = s'| S_t = s, A_t = a]\n",
    "$$\n",
    "\n",
    "Una **policy** $\\pi$ es una distribución de acciones de unos estados\n",
    "\n",
    "$$\n",
    "\\pi(a|s) = \\mathbb{P}[A_t = a | S_t=s]\n",
    "$$\n",
    "\n",
    "**state-value** para un MDP dada una policy\n",
    "\n",
    "$$\n",
    "\\mathcal{R}_s = \\mathbb{E_\\pi}[R_{t+1}|S_t = s] \n",
    "$$\n",
    "\n",
    "[**action-value**](https://youtu.be/lfHX2hHRMVQ?t=3160) para un MDP es el valor esperado dada una acción $a$ en el estado $s$\n",
    "\n",
    "$$\n",
    "q_\\pi(s,a) = \\mathbb{E_\\pi}[G_t|S_t=s, A_t=a] \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow 2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
